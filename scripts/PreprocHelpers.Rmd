---
title: "Helper Functions"
output: html_notebook
---

Libraries

```{r}
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(sesame))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(ComplexHeatmap))

suppressPackageStartupMessages(library(RColorBrewer))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(gridGraphics))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(dnaMethyAge))
suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(wateRmelon))
suppressPackageStartupMessages(library(methylclock))
suppressPackageStartupMessages(library(DunedinPACE))
```

Read dataset helper functions

```{r}
readXLSX <- function(file_path) {
  sheet_names <- excel_sheets(file_path)
  beta <- lapply(sheet_names, function(sheet) {
    as.data.table(read_excel(file_path, sheet=sheet))
  }) %>% rbindlist()
  
  return(beta)
}

readCSV <- function(file_path) {
  beta <- fread(file_path)
  return(beta)
}
```

Bound dataset helper function

```{r, verbose=FALSE}
check_bounds <- function(dataset) {
  temp <- na.omit(dataset)
  print(all(temp<=1 & temp>=0))
}

# Some datasets are beta values, but have values below 0 and above 1 due to already-done batch correction
limit_bounds <- function(dataset) {
  return(pmin(pmax(dataset, 0), 1))
}
```

Convert M to beta values helper function

```{r, verbose=FALSE}
m_to_b <- function(dataset) {
  dataset <- apply(dataset, c(1, 2), function(m) {
    2^m/(1+2^m)
  })
  return(dataset)
} 
```

mLiftOver

```{r, verbose=FALSE}
# Run this before
# sesameDataCache()

liftover <- function(dataset) {
  return(mLiftOver(dataset, "HM450", impute=F))
}
```

metadataTracker

```{r, verbose=FALSE}
age_range <- 0:100

# param updated: running metadata tracker, each age as rows, one column for each dataset, counting num samples at each age
# param md: current metadata file being examined
# param name: new column name in metadata tracker

# output: updated metadata tracker
# note: mostly used for examining sample distribution at each age across datasets

updateMetadata <- function(updated, md, name) {
  rounded_ages <- round(as.numeric(md$age))
  # ensure within bounds
  rounded_ages <- rounded_ages[rounded_ages <= 100]
  
  age_counts <- table(rounded_ages)
  dataset_column <- rep(0, length(age_range))
  names(dataset_column) <- paste0("age", age_range)
  
  for (age in names(age_counts)) {
    dataset_column[paste0("age", as.integer(age))] <- age_counts[age]
  }
  
  updated <- cbind(updated, dataset_column)
  colnames(updated)[ncol(updated)] <- name
  return(updated)
}
```

update valid samples
- non-NA age
- age <= 80 (or other if threshold shifts)

```{r, verbose=FALSE}
# param valid_samples: running list of samples with non-NA and <=80 age
# param md: current metadata file being examined
# param age_threshold: age cutoff to be valid, set at 80 as specified before

# output: updated list of valid samples

updateValidSamples <- function(valid_samples, md, age_threshold) {
  temp <- c(valid_samples, md[as.numeric(md$age) <= age_threshold, ]$sample_id)
  return(temp)
}
```

Make splits
Note: I am only doing this because of computational limitations... it makes it much more tedious
If skipping this step, pipeline can be resumed at filtering (matrix generation function relies on splits)

```{r}
library(data.table)

# param cpg_list: from previous analyses, number of CpG sites present in X/total datasets, as of 1/9/25, it is 16/20 (subject to change)
# param complete_metadata: combined metadata from all datasets: "sample_id", "age", "gender", "type", "tissue" (or cell type), "disease_status"
# param valid_samples: all samples with non-NA or <=80 ages
# param sites_per_list: size of splits, up to 50,000 now due to purchase of better laptop
# param output_folder: folder to write splits to

# output: none, writes N splits to project files

makeSplits <- function(cpg_list, full_metadata, valid_samples, sites_per_list, output_folder) {
  # List all preprocessed datasets
  file_list <- list.files(path = "../data/processed")
  
  NUMCPGSITE <- length(cpg_list)
  num_groups <- ceiling(NUMCPGSITE/sites_per_list)
  
  # Split cpg sites
  split_lists <- vector("list", num_groups)
  for (i in 1:num_groups) {
    start_index <- (i-1)*sites_per_list + 1
    end_index <- min(i*sites_per_list, NUMCPGSITE)
    split_lists[[i]] <- cpg_list[start_index:end_index]
  }
  
  # Loop through each group of CpG sites
  for (i in 1:length(split_lists)) {
    template_sites <- c("age", split_lists[[i]])
    temp <- data.frame(matrix(NA, nrow=length(template_sites), ncol=0), row.names = template_sites)
    
    for (j in 1:length(file_list)) {
      df <- fread(paste0("../data/processed/", file_list[j]))
      print(paste0("Looking at dataset: ", file_list[j]))
      df <- as.data.frame(df)
      rownames(df) <- df[, 1]
      df <- df[, -1]
      
      # Filter down only to sites being examined
      df <- df[rownames(df) %in% template_sites, ]
      
      # Filter down to valid samples
      df <- df[, colnames(df) %in% valid_samples]
      
      # format metadata to append
      cor_metadata <- full_metadata[full_metadata$sample_id %in% colnames(df), c("sample_id", "age")]
      cor_metadata <- t(cor_metadata)
      colnames(cor_metadata) <- cor_metadata[1, ]
      cor_metadata <- cor_metadata[-1, , drop=FALSE]
      # reordering
      cor_metadata <- cor_metadata[, colnames(df), drop=FALSE]
      
      df <- rbind(cor_metadata, df)
      
      df <- df[template_sites, , drop = FALSE]
      
      temp <- cbind(temp, df)
    }
    fwrite(temp, paste0(output_folder, "split_", i, ".csv"), row.names=TRUE)
    rm(temp)
    gc()
  }
}
```

Make matrix
- at this point, splits are already filtered to valid sites and valid samples

```{r}
# param row_number: self explanatory
# param cpg_list: row names
# param start_age: self explanatory
# param end_age: self explanatory

# output: creates an empty matrix with CpG sites as rows and ages as columns

createEmptyMatrix <- function(row_number, cpg_list, start_age, end_age) {
  temp <- matrix(NA, nrow=row_number, ncol = end_age - start_age + 1)
  age_range <- start_age:end_age
  rownames(temp) <- cpg_list
  # column names as "age_X"
  column_names <- character(end_age - start_age + 1)
  for (i in 1:(end_age - start_age + 1)) {
    column_names[i] <- paste0("age_", i-1+start_age)
  }
  colnames(temp) <- column_names
  return(temp)
}

# Generate overall matrix for downstream analysis
# param input_folder: folder to read splits from
# param cpg_list: list of desired cpgs for the matrix
# param full_metadata: metadato to reference
# param gender: "all", "male", or "female": specify which samples to include
# param use_clock: only include clock CpGs or not
# param average: if true, generate matrix of averages, if false, generate matrix of standard deviations
# param output_folder: folder to output matrices

# output: matrix for downstream analysis

generateMatrix <- function(input_folder, cpg_list, full_metadata, gender, average, output_folder) {
  list_files <- list.files(path = input_folder)
  
  num_rows <- length(cpg_list)
  
  B <- createEmptyMatrix(num_rows, cpg_list, 0, 80)
  
  for (i in 1:length(list_files)) {
    temp <- fread(paste0(input_folder, list_files[i]))
    temp <- as.data.frame(temp)
    rownames(temp) <- temp[, 1]
    temp <- temp[, -1]
    
    if (gender=="male") {
      temp <- temp[, colnames(temp) %in% male_samples]
    } else if (gender=="female") {
      temp <- temp[, colnames(temp) %in% female_samples]
    }
    
    print(dim(temp))
    
    # ensure numeric & round ages to nearest integer
    if (all(sapply(temp, is.numeric))) {
      print(".")
    } else {
      print("not all numeric")
      break
    }
    temp["age", ] <- round(temp["age", ])
    temp <- temp[rownames(temp) %in% c(cpg_list, "age"), ]
    
    ident <- rownames(temp[-1, ])   
    
    for (j in 0:80) {
      # Select samples at age j
      des_cols <- which(temp["age", ]==j)
      
      # Check if there are samples at age j
      if (length(des_cols) != 0) {
        temp2 <- temp[, des_cols]
        
        if (is.null(nrow(temp2)) || nrow(temp2) < 2) {
          print(paste0("Skipping age ", j, " in Split ", i, " due to insufficient data"))
          next
        }
        
        # Remove Age Row
        temp2 <- temp2[-1, ]
        
        setDT(temp2)
        
        # Generate averages and standard deviations at age j for vector of specified CpG sites
        
        if (average) {
          average_vector <- temp2[, .(final = rowMeans(.SD, na.rm=TRUE)), by=.I]
          average_vector <- as.data.frame(average_vector)
          B[ident, paste0("age_", j)] <- average_vector$final
        } else {
          sd_vector <- temp2[, .(final = apply(.SD, 1, sd, na.rm=TRUE)), by=.I]
          sd_vector <- as.data.frame(sd_vector)
          B[ident, paste0("age_", j)] <- sd_vector$final
        }
            
        # Logging
        print(paste0("Processing age ", j, ", Split ", i))
      } else {
        print(paste0("No samples at age ", j))
      }
    }
  }
  
  file_name <- paste0(output_folder, gender, average, ".csv")
  fwrite(B, file_name, row.names=TRUE)
}
```

Make vert chunks (used in scenarios where we want it split by sites and not samples)

```{r}
# Reformat splits from (50000 sites x all samples) to (all sites x 1000 samples) to ensure that samples have all available sites for EAA calculation
file_list <- list.files(path="../data/splits")

all_data <- data.table()
chunk_size <- 1000
temp <- fread("../data/splits/split_1.csv", nrows = 1)
total_columns <- ncol(temp) - 1  # Adjust for removed first column
num_chunks <- ceiling(total_columns / chunk_size)

for (chunk_index in 1:num_chunks) {
  chunk_data <- data.table()
  
  for (i in 1:length(file_list)) {
    # Read the current file
    temp <- fread(paste0("../data/splits/split_", i, ".csv"))

    # Remove the first row (age information)
    temp <- temp[-1, ]
    # Store first col (cpg identifiers)
    first_col <- temp[, 1, drop=FALSE]
    # Drop first col
    temp <- temp[, -1]

    # Determine the start and end columns for the current chunk
    start_col <- (chunk_index - 1) * chunk_size + 1
    end_col <- min(chunk_index * chunk_size, ncol(temp))
    
    # If the chunk is within the range of the current file, process it
    if (start_col <= ncol(temp)) {
      # Extract the columns for the current chunk
      temp_chunk <- temp[, start_col:end_col, with = FALSE]
      temp_chunk <- cbind(first_col, temp_chunk)
      
      # Combine the current chunk with previous file chunks
      chunk_data <- rbindlist(list(chunk_data, temp_chunk), use.names = TRUE, fill = TRUE)
    }
  }
  # Save the combined chunk data to a temporary file
  fwrite(chunk_data, paste0("../data/vert_splits/vert_split_", chunk_index, ".csv"))
}
```

Plot single CpG given some parameters
specify seg_start and seg_end for additional annotation (regression line of a window)

```{r}
set.seed(123)
# Create split lists
# Read desired site list
sites <- readRDS("../intermediates/site_list_by_samples.rds")
cpg_list <- rownames(sites)
NUMCPGSITE <- length(cpg_list)
sites_per_list <- 50000
num_groups <- ceiling(NUMCPGSITE/sites_per_list)

# Create splits
split_lists <- vector("list", num_groups)
for (i in 1:num_groups) {
  start_index <- (i-1)*sites_per_list + 1
  end_index <- min(i*sites_per_list, NUMCPGSITE)
  split_lists[[i]] <- cpg_list[start_index:end_index]
}

plotCpG <- function(cpg_list, gender_state, seg_start, seg_end) {
  plot_list2 <- list()
  
  # Check which split to extract from in my files
  for (cpg_name in cpg_list) {
    splits <- list.files(path="../data/splits")
    
    # Find which split
    num <- 0
    for (i in 1:length(split_lists)) {
      if (cpg_name %in% split_lists[[i]]) {
        num <- i
      }
    }
    temp <- fread(paste0("../data/splits/split_", num, ".csv"))
    temp <- as.data.frame(temp)
    
    rownames(temp) <- temp$V1
    temp <- temp[, -c(1, 2)]
    
    # Filter for samples
    # all_samples, male_samples, female_samples
    
    
    temp <- temp[c("age", cpg_name), ]
    
    temp <- as.data.frame(t(temp))
    
    colnames(temp) <- c("Age", "Beta")
    temp <- temp[temp$Age<=80, ]
    
    temp_window <- temp[temp$Age >= seg_start & temp$Age <= seg_end, ]
    correlation <- cor(temp_window$Age, temp_window$Beta, use="complete.obs")
    correlation <- round(correlation, digits=2)
    
    a <- ggscatter(temp, x = "Age", y = "Beta", size = 1, title = cpg_name,
                     xlab = "Age", ylab = "Beta Values", color = "lightgray", shape=16) +
        #geom_smooth(method = "loess", color = "blue", fill = "lightblue", se = FALSE) +
        geom_smooth(data = temp_window, method = "lm", se = FALSE, color = "blue") + 
        ylim(0, 1) +
        xlim(0, 80) + 
    theme_and_axis_nolegend
    a <- a + annotate("text", x = 70, y = 1,
      label = paste0("r = ", correlation), color="red", fontface="bold")
    
    plot_list2[[length(plot_list2) + 1]] <- a
    
    # if (save==TRUE) {
    #   ggsave(filename = paste0("figs/", cpg_name, ".png"), plot = a, height=6, width=10, dpi = 300)
    # }
  }
  return(plot_list2)
}
```


Additional dependencies needed in Preproc

```{r}
# Read standard CpG site list to be used (not until matrix has been generated then filtered)
ann <- read.table("../data/ref/HM450.hg38.manifest.tsv", sep="\t", header=TRUE, quote="", comment.char="", fileEncoding = "UTF-8")
standard_cpg_list <- ann$probeID

# ggplot parameters here

theme_set(theme_pubr())
MALE_COLOR <- "#ADD8E6"
FEMALE_COLOR <- "#FFB6C1"
ALL_COLOR <- "black"

# theme_and_axis_nolegend <- theme(
#     legend.position = "none",       
#     text = element_text(face = "bold"), 
#     axis.title = element_text(face = "bold", size=30), 
#     axis.text = element_text(face = "bold", size=20),  
#     plot.title = element_text(face = "bold", hjust=0.5, size=40),  
#     plot.subtitle = element_text(face = "bold") 
#   )

theme_and_axis_nolegend <- theme(
    legend.position = "none",       
    text = element_text(face = "bold"), 
    axis.title = element_text(face = "bold"), 
    axis.text = element_text(face = "bold"),  
    plot.title = element_text(face = "bold", hjust=0.5),  
    plot.subtitle = element_text(face = "bold") 
  )

theme_and_axis_legend <- theme(
    legend.position = "right",        
    text = element_text(face = "bold"), 
    axis.title = element_text(face = "bold"), 
    axis.text = element_text(face = "bold"),  
    plot.title = element_text(face = "bold", hjust = 0.5),  
    plot.subtitle = element_text(face = "bold"),
    legend.text = element_text(face = "bold"),
      legend.box.background = element_rect(color = "black"),
      theme(legend.key.size = unit(10, "cm")), # not too sure how this works
    legend.title = element_text(size = 12, face = "bold", hjust = 0.5, family="Helvetica")
)

# Get clock sites
Hannum_CpGs <- read_excel("../data/ref/CpGsToInvestigate/hannum_cpgs.xlsx")
Hannum_CpGs <- Hannum_CpGs$Marker #71
Levine_CpGs <- read.csv("../data/ref/CpGsToInvestigate/levine_cpgs.csv", stringsAsFactors=FALSE)
Levine_CpGs <- Levine_CpGs[-1, ]
Levine_CpGs <- Levine_CpGs$CpG #513
Horvath_CpGs <- read.csv("../data/ref/CpGsToInvestigate/horvath_cpgs.csv", stringsAsFactors=FALSE)
Horvath_CpGs <- Horvath_CpGs[-(1:3), 1, drop=FALSE]
Horvath_CpGs <- Horvath_CpGs[, 1] #353
McEwen_CpGs <- read.csv("../data/ref/CpGsToInvestigate/mcewen_cpgs.csv")
McEwen_CpGs <- McEwen_CpGs$CPG #94
Wu_CpGs <- read_excel("../data/ref/CpGsToInvestigate/aging-11-102399-s003..xlsx")
Wu_CpGs <- Wu_CpGs[-1, ]
Wu_CpGs <- Wu_CpGs$CpGs #111
Belsky_CpGs <- getRequiredProbes(backgroundList=FALSE)
Belsky_CpGs <- unlist(Belsky_CpGs) #173
load("../data/ref/CpGsToInvestigate/epitoc.Rd")
Teschendorff_CpGs <- as.data.frame(dataETOC2.l[1])
Teschendorff_CpGs <- rownames(Teschendorff_CpGs) #163
Shireby_CpGs <- readLines("../data/ref/CpGsToInvestigate/CorticalClockCoefs.txt")[-1]
Shireby_CpGs <- sapply(strsplit(Shireby_CpGs, " "), `[`, 1)
Shireby_CpGs <- unlist(Shireby_CpGs) #347
Lu2_CpGs <- read_excel("../data/ref/CpGsToInvestigate/lu2.xlsx")
Lu2_CpGs <- rbind(colnames(Lu2_CpGs), Lu2_CpGs)
Lu2_CpGs <- Lu2_CpGs[, 1]
Lu2_CpGs <- as.character(Lu2_CpGs[[1]]) #140
clock_cpgs <- list(
  McEwen = McEwen_CpGs,
  Wu = Wu_CpGs,
  Hannum = Hannum_CpGs,
  Horvath = Horvath_CpGs,
  Levine = Levine_CpGs,
  Belsky = Belsky_CpGs,
  Teschendorff = Teschendorff_CpGs,
  Shireby = Shireby_CpGs,
  Lu = Lu2_CpGs
)

cpgs <- unlist(clock_cpgs)
cpgs <- unique(cpgs) #1868 total sites

print(paste0("Total Unique CpG sites from 9 clocks explored: ", length(cpgs)))

#write.csv(cpgs, "7_3_overlap.csv")

# Filter for clock sites above threshold
filtered_sites <- readRDS("../intermediates/POST_sites.rds")

# 1100 clock cpgs only
filtered_clock_cpgs <- cpgs[cpgs %in% filtered_sites]

for (i in 1:length(clock_cpgs)) {
  clock_cpgs[[i]] <- intersect(clock_cpgs[[i]], filtered_clock_cpgs)
}

# UpSet plot for clock site overlap 
# Will modify aesthetics to match when aesthetics for previous plots are finalized.

m1 <- make_comb_mat(clock_cpgs, mode="distinct")
clock_upset_grob <- grid.grabExpr({
  grid.newpage()
  draw(UpSet(m1, set_order = rownames(m1), pt_size = unit(1.5, "mm"), lwd=1))
})
```

